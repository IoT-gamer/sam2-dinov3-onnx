{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPijLIq1a5q5IEgQp814zwx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoT-gamer/segment-anything-dinov3-onnx/blob/main/notebooks/dinov3_one_shot_segmentation_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One-shot Segmentation by Feature Matching using DINOv3\n",
        "\n",
        "- The DINOv3 features are so powerful that they can identify semantically similar regions across different images without any extra training."
      ],
      "metadata": {
        "id": "k_vTRd6QuaBZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Concept\n",
        "1. **Create a \"Fingerprint\":** Extract all the DINOv3 feature vectors from your single reference image. Using its alpha mask, isolate only the features that belong to the foreground object. The average of these features becomes the \"fingerprint\" or prototype vector for your target object.\n",
        "\n",
        "2. **Scan for Matches:** Extract all the feature vectors from the test image.\n",
        "\n",
        "3. **Compare:** Calculate the similarity (specifically, the cosine similarity) between the object's prototype vector and every single feature vector in the test image.\n",
        "\n",
        "4. **Visualize:** This comparison results in a similarity map where high values indicate a strong match to your reference object. This map is your final segmentation."
      ],
      "metadata": {
        "id": "zUBNYj8cvQuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisites\n",
        "- This notebook uses DINOv3 feature extractor model exported to ONNX using [dinov3_onnx_export.ipynb](https://github.com/IoT-gamer/segment-anything-dinov3-onnx/blob/main/notebooks/dinov3_onnx_export.ipynb)\n",
        "- Reference image should have segmented mask in alpha-channel of RBGA .png file\n",
        "  - for example, use [flutter_segment_anything_app](https://github.com/IoT-gamer/flutter_segment_anything_app)"
      ],
      "metadata": {
        "id": "AXrGiDdvwD24"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model and Reference Image\n",
        "- Load the ONNX feature extractor session and your single reference RGBA image."
      ],
      "metadata": {
        "id": "ldI9Pgubb1BN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ],
      "metadata": {
        "id": "MPXOUqE0aRtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "W0NLJHQyaWqp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if storing model and/or images in google drive and using google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Zzmc3k9CE6TJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as ort\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "KUCf_7EHaCWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ],
      "metadata": {
        "id": "wC6XWYqLaaDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = 768\n",
        "PATCH_SIZE = 16\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)"
      ],
      "metadata": {
        "id": "4aVaHEL7aHoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the ONNX Feature Extractor\n",
        "\n",
        "- Export DINOv3 feature extractor onnx model using this notebook:\n",
        "  - [dinov3_onnx_export.ipynb](https://github.com/IoT-gamer/segment-anything-dinov3-onnx/blob/main/notebooks/dinov3_onnx_export.ipynb)"
      ],
      "metadata": {
        "id": "RJNef9EUagOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_feature_extractor_path = \"dinov3_feature_extractor.onnx\"\n",
        "feature_extractor_session = ort.InferenceSession(onnx_feature_extractor_path)"
      ],
      "metadata": {
        "id": "P07sLREiaPzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Reference Image\n",
        "- Reference image should be an RBGA .png file with mask stored in alpha-channel"
      ],
      "metadata": {
        "id": "46DVzm3naryg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ref_image_path = \"/path/to/your/single_reference_rgba.png\"\n",
        "ref_image_rgba = Image.open(ref_image_path)\n",
        "\n",
        "# Separate image and mask\n",
        "ref_image_rgb = ref_image_rgba.convert(\"RGB\")\n",
        "ref_mask = ref_image_rgba.split()[-1]"
      ],
      "metadata": {
        "id": "UVaEVrDAarOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create the Object Prototype\n",
        "- Process the reference image to create the average feature vector for the foreground object"
      ],
      "metadata": {
        "id": "xIPYhxboenZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Preprocessing and Get Features"
      ],
      "metadata": {
        "id": "EIvUD_gArk1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_for_prototyping(img_pil, mask_pil, image_size, patch_size):\n",
        "    # Preprocess the RGB image\n",
        "    w, h = img_pil.size\n",
        "    h_patches = image_size // patch_size\n",
        "    w_patches = int((w * image_size) / (h * patch_size))\n",
        "    if w_patches % 2 != 0: w_patches -= 1\n",
        "\n",
        "    new_h, new_w = h_patches * patch_size, w_patches * patch_size\n",
        "    resized_img = img_pil.resize((new_w, new_h), Image.Resampling.BICUBIC)\n",
        "\n",
        "    img_np = np.array(resized_img, dtype=np.float32) / 255.0\n",
        "    mean = np.array(IMAGENET_MEAN, dtype=np.float32)\n",
        "    std = np.array(IMAGENET_STD, dtype=np.float32)\n",
        "    normalized_img = (img_np - mean) / std\n",
        "    input_tensor = normalized_img.transpose(2, 0, 1)[np.newaxis, :, :]\n",
        "\n",
        "    # Downsample the mask to match the patch grid\n",
        "    resized_mask = mask_pil.resize((w_patches, h_patches), Image.Resampling.NEAREST)\n",
        "    mask_np = np.array(resized_mask, dtype=np.float32) / 255.0\n",
        "    patch_mask = (mask_np > 0.5).flatten() # Flatten to a 1D boolean array\n",
        "\n",
        "    return input_tensor, patch_mask, (h_patches, w_patches)\n",
        "\n",
        "# Preprocess and get features\n",
        "ref_input_tensor, ref_patch_mask, _ = preprocess_for_prototyping(ref_image_rgb, ref_mask, IMAGE_SIZE, PATCH_SIZE)\n",
        "ref_inputs = {feature_extractor_session.get_inputs()[0].name: ref_input_tensor}\n",
        "ref_features = feature_extractor_session.run(None, ref_inputs)[0].squeeze()"
      ],
      "metadata": {
        "id": "ox6b61DuaznS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the prototype"
      ],
      "metadata": {
        "id": "J1lKvZYVuBa7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select only the features corresponding to the foreground mask\n",
        "foreground_features = ref_features[ref_patch_mask]\n",
        "# Calculate the mean feature vector (the \"fingerprint\")\n",
        "object_prototype = np.mean(foreground_features, axis=0, keepdims=True)\n",
        "\n",
        "print(f\"Created object prototype with shape: {object_prototype.shape}\")\n",
        "# Expected output: Created object prototype with shape: (1, 384) for vits16"
      ],
      "metadata": {
        "id": "QubmQPXZuEpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Process a Test Image and Compute Similarity\n",
        "-  Load a new test image, extract its features, and compute the cosine similarity against the prototype"
      ],
      "metadata": {
        "id": "JPW_AGPYsIJF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and Preprocess Test Image"
      ],
      "metadata": {
        "id": "ncWGMKrtsqkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_path = \"/path/to/your/test_image.jpg\"\n",
        "test_image = Image.open(test_image_path).convert('RGB')\n",
        "\n",
        "def preprocess_image_numpy(img_pil, image_size, patch_size):\n",
        "    \"\"\"Resizes and normalizes an image using NumPy and Pillow.\"\"\"\n",
        "    w, h = img_pil.size\n",
        "    h_patches = image_size // patch_size\n",
        "    w_patches = int((w * image_size) / (h * patch_size))\n",
        "    if w_patches % 2 != 0: w_patches -= 1\n",
        "\n",
        "    new_h, new_w = h_patches * patch_size, w_patches * patch_size\n",
        "    resized_img = img_pil.resize((new_w, new_h), Image.Resampling.BICUBIC)\n",
        "\n",
        "    img_np = np.array(resized_img, dtype=np.float32) / 255.0\n",
        "    mean = np.array(IMAGENET_MEAN, dtype=np.float32)\n",
        "    std = np.array(IMAGENET_STD, dtype=np.float32)\n",
        "    normalized_img = (img_np - mean) / std\n",
        "\n",
        "    input_tensor = normalized_img.transpose(2, 0, 1)[np.newaxis, :, :]\n",
        "    return input_tensor, (h_patches, w_patches), resized_img\n",
        "\n",
        "test_input_tensor, patch_dims, resized_test_image = preprocess_image_numpy(test_image, IMAGE_SIZE, PATCH_SIZE)\n",
        "h_patches, w_patches = patch_dims\n",
        "\n"
      ],
      "metadata": {
        "id": "XzxWqWFQsi7n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Test Image Features"
      ],
      "metadata": {
        "id": "OW4PVlWCs961"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_inputs = {feature_extractor_session.get_inputs()[0].name: test_input_tensor}\n",
        "test_features = feature_extractor_session.run(None, test_inputs)[0].squeeze()"
      ],
      "metadata": {
        "id": "xGbMtZaXtHKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute Similarity Map"
      ],
      "metadata": {
        "id": "sWUxFkYsteG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the object prototype to every patch feature in the test image\n",
        "similarity_scores = cosine_similarity(test_features, object_prototype)\n",
        "similarity_map = similarity_scores.reshape(h_patches, w_patches)"
      ],
      "metadata": {
        "id": "RQFVa22EtiP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize the Result\n"
      ],
      "metadata": {
        "id": "C4qHiMyttnus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 5), dpi=150)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(resized_test_image)\n",
        "plt.title('Test Image'); plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(similarity_map, cmap='viridis')\n",
        "plt.title('Object Similarity Map'); plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "24FCev5nttYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Further Improvements\n",
        "- **Segment Anything:** feed x-y coordinates from the similarity map into a segment-anything model [edgetam_onnx_export.ipynb](https://github.com/IoT-gamer/segment-anything-dinov3-onnx/blob/main/notebooks/edgetam_onnx_export.ipynb)\n",
        "- **Multi-Prototype Matching:** If your object has very distinct parts (e.g., a person with a red hat and a blue shirt), averaging all features into one prototype might dilute the signal. You could use an algorithm like K-Means on the foreground_features to find 2 or 3 cluster centers (prototypes). Then, for each patch in the test image, you would find its similarity to the closest of these prototypes. This often yields sharper results."
      ],
      "metadata": {
        "id": "r_Hf2ik0zUTC"
      }
    }
  ]
}