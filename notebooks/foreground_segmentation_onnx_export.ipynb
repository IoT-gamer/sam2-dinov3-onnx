{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoT-gamer/sam2-dinov3-onnx/blob/main/notebooks/foreground_segmentation_onnx_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9966b9cd",
      "metadata": {
        "id": "9966b9cd"
      },
      "source": [
        "# Training a Foreground Segmentation Tool with DINOv3\n",
        "## Export Model in ONNX Format\n",
        "\n",
        "- train a linear foreground segmentation model using DINOv3 features.\n",
        "- export DINOv3 Feature Extractor to ONNX\n",
        "- export Logistic Regression Classifier to ONNX"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Acknoweldgements/References\n",
        "- [DINOv3 github repo](https://github.com/facebookresearch/dinov3)\n",
        "- [foreground_segmentation.ipynb](https://github.com/facebookresearch/dinov3/blob/main/notebooks/foreground_segmentation.ipynb)\n",
        "  - Model Training taken from this notebook\n",
        "  - The main modification is the export to ONNX section"
      ],
      "metadata": {
        "id": "U0M3Au81HbQS"
      },
      "id": "U0M3Au81HbQS"
    },
    {
      "cell_type": "markdown",
      "id": "a03c01a4-2fe0-4d61-b05c-9d3274bda2df",
      "metadata": {
        "id": "a03c01a4-2fe0-4d61-b05c-9d3274bda2df"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import tarfile\n",
        "import urllib\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "4_KnxlWnGZtf"
      },
      "id": "4_KnxlWnGZtf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "60692f17",
      "metadata": {
        "id": "60692f17"
      },
      "source": [
        "### Model\n",
        "\n",
        "- visit https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/ to get weights URLs\n",
        "- add a secret named `dinov3_vits16` and copy and past the `dinov3_vits16` URL in the value\n",
        "- note: the URLs may expire after a few days.\n",
        "  - need to save or re-register"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e91ecf2c",
      "metadata": {
        "scrolled": true,
        "id": "e91ecf2c"
      },
      "outputs": [],
      "source": [
        "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
        "MODEL_NAME = MODEL_DINOV3_VITS\n",
        "\n",
        "WEIGHTS_URL = userdata.get('dinov3_vits16') # URL is stored in colab secrets\n",
        "\n",
        "model = torch.hub.load(\n",
        "    repo_or_dir=\"facebookresearch/dinov3\",\n",
        "    model=MODEL_NAME,\n",
        "    source=\"github\",\n",
        "    weights=WEIGHTS_URL\n",
        ")\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad3a9aab",
      "metadata": {
        "id": "ad3a9aab"
      },
      "source": [
        "### Data\n",
        "Now that we have the model set up, let's load the training data. It consists of:\n",
        "\n",
        "- images in `jpg` format:\n",
        "```\n",
        "https://dl.fbaipublicfiles.com/dinov3/notebooks/foreground_segmentation/foreground_segmentation_images.tar.gz\n",
        "```\n",
        "\n",
        "- and segmentation masks stored as alpha channels in `png` files:\n",
        "```\n",
        "https://dl.fbaipublicfiles.com/dinov3/notebooks/foreground_segmentation/foreground_segmentation_labels.tar.gz\n",
        "```\n",
        "\n",
        "In total, there are 9 training image / mask pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc40dc59",
      "metadata": {
        "id": "dc40dc59"
      },
      "outputs": [],
      "source": [
        "IMAGES_URI = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/foreground_segmentation/foreground_segmentation_images.tar.gz\"\n",
        "LABELS_URI = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/foreground_segmentation/foreground_segmentation_labels.tar.gz\"\n",
        "\n",
        "def load_images_from_remote_tar(tar_uri: str) -> list[Image.Image]:\n",
        "    images = []\n",
        "    with urllib.request.urlopen(tar_uri) as f:\n",
        "        tar = tarfile.open(fileobj=io.BytesIO(f.read()))\n",
        "        for member in tar.getmembers():\n",
        "            image_data = tar.extractfile(member)\n",
        "            image = Image.open(image_data)\n",
        "            images.append(image)\n",
        "    return images\n",
        "\n",
        "images = load_images_from_remote_tar(IMAGES_URI)\n",
        "labels = load_images_from_remote_tar(LABELS_URI)\n",
        "n_images = len(images)\n",
        "assert n_images == len(labels), f\"{len(images)=}, {len(labels)=}\"\n",
        "\n",
        "print(f\"Loaded {n_images} images and labels\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc6eb95-bd20-4e54-a967-81304aac9866",
      "metadata": {
        "id": "4dc6eb95-bd20-4e54-a967-81304aac9866"
      },
      "source": [
        "Let's, for example, visualize the first image / mask pair:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48333927-9ed4-4f32-beb9-f3d57fc6574d",
      "metadata": {
        "id": "48333927-9ed4-4f32-beb9-f3d57fc6574d"
      },
      "outputs": [],
      "source": [
        "data_index = 0\n",
        "\n",
        "print(f\"Showing image / mask at index {data_index}:\")\n",
        "\n",
        "image = images[data_index]\n",
        "mask = labels[data_index]\n",
        "foreground = Image.composite(image, mask, mask)\n",
        "mask_bg_np = np.copy(np.array(mask))\n",
        "mask_bg_np[:, :, 3] = 255 - mask_bg_np[:, :, 3]\n",
        "mask_bg = Image.fromarray(mask_bg_np)\n",
        "background = Image.composite(image, mask_bg, mask_bg)\n",
        "\n",
        "data_to_show = [image, mask, foreground, background]\n",
        "data_labels = [\"Image\", \"Mask\", \"Foreground\", \"Background\"]\n",
        "\n",
        "plt.figure(figsize=(16, 4), dpi=300)\n",
        "for i in range(len(data_to_show)):\n",
        "    plt.subplot(1, len(data_to_show), i + 1)\n",
        "    plt.imshow(data_to_show[i])\n",
        "    plt.axis('off')\n",
        "    plt.title(data_labels[i], fontsize=12)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54002c83",
      "metadata": {
        "id": "54002c83"
      },
      "source": [
        "### Building Per-Patch Label Map\n",
        "\n",
        "Since our models run with a patch size of 16, we have to quantize the ground truth to a 16x16 pixels grid. To achieve this, we define:\n",
        "- the resize transform to resize an image such that it aligns well with the 16x16 grid;\n",
        "- a uniform 16x16 conv layer as a [box blur filter](https://en.wikipedia.org/wiki/Box_blur) with stride equal to the patch size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4317cae",
      "metadata": {
        "id": "b4317cae"
      },
      "outputs": [],
      "source": [
        "PATCH_SIZE = 16\n",
        "IMAGE_SIZE = 768\n",
        "\n",
        "# quantization filter for the given patch size\n",
        "patch_quant_filter = torch.nn.Conv2d(1, 1, PATCH_SIZE, stride=PATCH_SIZE, bias=False)\n",
        "patch_quant_filter.weight.data.fill_(1.0 / (PATCH_SIZE * PATCH_SIZE))\n",
        "\n",
        "# image resize transform to dimensions divisible by patch size\n",
        "def resize_transform(\n",
        "    mask_image: Image,\n",
        "    image_size: int = IMAGE_SIZE,\n",
        "    patch_size: int = PATCH_SIZE,\n",
        ") -> torch.Tensor:\n",
        "    w, h = mask_image.size\n",
        "    h_patches = int(image_size / patch_size)\n",
        "    w_patches = int((w * image_size) / (h * patch_size))\n",
        "    return TF.to_tensor(TF.resize(mask_image, (h_patches * patch_size, w_patches * patch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab9673f-4a6f-43ca-8ba2-2c6ca39ff460",
      "metadata": {
        "id": "1ab9673f-4a6f-43ca-8ba2-2c6ca39ff460"
      },
      "source": [
        "Let's, for example, visualize the first mask before and after quantization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32657c30-8f71-46bb-a2f5-c7f47d3e94d1",
      "metadata": {
        "id": "32657c30-8f71-46bb-a2f5-c7f47d3e94d1"
      },
      "outputs": [],
      "source": [
        "mask_0 = labels[0].split()[-1]\n",
        "mask_0_resized = resize_transform(mask_0)\n",
        "with torch.no_grad():\n",
        "    mask_0_quantized = patch_quant_filter(mask_0_resized).squeeze().detach().cpu()\n",
        "\n",
        "plt.figure(figsize=(4, 2), dpi=300)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(mask_0)\n",
        "plt.axis('off')\n",
        "plt.title(f\"Original Mask, Size {mask_0.size}\", fontsize=5)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(mask_0_quantized)\n",
        "plt.axis('off')\n",
        "plt.title(f\"Quantized Mask, Size {tuple(mask_0_quantized.shape)}\", fontsize=5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0763e21",
      "metadata": {
        "id": "d0763e21"
      },
      "source": [
        "### Extracting Features and Labels for All the Images\n",
        "Now we will loop over the 9 training images, and extract for each image the patch labels, as well as the patch features. That involves running the dense feature extraction of our model with :\n",
        "\n",
        "```\n",
        "with torch.no_grad():        \n",
        "    feats = model.get_intermediate_layers(img, n=range(n_layers), reshape=True, norm=True)\n",
        "    dim = feats[-1].shape[1]\n",
        "    xs.append(feats[-1].squeeze().view(dim, -1).permute(1,0).detach().cpu())\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a47d6a1b",
      "metadata": {
        "id": "a47d6a1b"
      },
      "outputs": [],
      "source": [
        "xs = []\n",
        "ys = []\n",
        "image_index = []\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "MODEL_TO_NUM_LAYERS = {\n",
        "    MODEL_DINOV3_VITS: 12,\n",
        "    # MODEL_DINOV3_VITSP: 12,\n",
        "    # MODEL_DINOV3_VITB: 12,\n",
        "    # MODEL_DINOV3_VITL: 24,\n",
        "    # MODEL_DINOV3_VITHP: 32,\n",
        "    # MODEL_DINOV3_VIT7B: 40,\n",
        "}\n",
        "\n",
        "n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
        "        for i in tqdm(range(n_images), desc=\"Processing images\"):\n",
        "            # Loading the ground truth\n",
        "            mask_i = labels[i].split()[-1]\n",
        "            mask_i_resized = resize_transform(mask_i)\n",
        "            mask_i_quantized = patch_quant_filter(mask_i_resized).squeeze().view(-1).detach().cpu()\n",
        "            ys.append(mask_i_quantized)\n",
        "            # Loading the image data\n",
        "            image_i = images[i].convert('RGB')\n",
        "            image_i_resized = resize_transform(image_i)\n",
        "            image_i_resized = TF.normalize(image_i_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "            image_i_resized = image_i_resized.unsqueeze(0).cuda()\n",
        "\n",
        "            feats = model.get_intermediate_layers(image_i_resized, n=range(n_layers), reshape=True, norm=True)\n",
        "            dim = feats[-1].shape[1]\n",
        "            xs.append(feats[-1].squeeze().view(dim, -1).permute(1,0).detach().cpu())\n",
        "\n",
        "            image_index.append(i * torch.ones(ys[-1].shape))\n",
        "\n",
        "\n",
        "# Concatenate all lists into torch tensors\n",
        "xs = torch.cat(xs)\n",
        "ys = torch.cat(ys)\n",
        "image_index = torch.cat(image_index)\n",
        "\n",
        "# keeping only the patches that have clear positive or negative label\n",
        "idx = (ys < 0.01) | (ys > 0.99)\n",
        "xs = xs[idx]\n",
        "ys = ys[idx]\n",
        "image_index = image_index[idx]\n",
        "\n",
        "print(\"Design matrix of size : \", xs.shape)\n",
        "print(\"Label matrix of size : \", ys.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fade40b7",
      "metadata": {
        "id": "fade40b7"
      },
      "source": [
        "### Training a Classifier and Model Selection\n",
        "We computed the features, let's now train a classifier! Our data is very strongly correlated image-by-image. Therefore, to do proper model selection, we can't simply split the data in an IID way. We need to do something a bit smarter. We will do leave-one-out, and consecutively exclude each image as a validation set.\n",
        "We'll try 8 values of C ranging from 1e-7 to 1e-0.\n",
        "\n",
        "For each value of C and each image, we plot the precision-recall curve of the classifier, and report the mAP (area under the PR curve)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f92f6714",
      "metadata": {
        "id": "f92f6714"
      },
      "outputs": [],
      "source": [
        "cs = np.logspace(-7, 0, 8)\n",
        "scores = np.zeros((n_images, len(cs)))\n",
        "\n",
        "for i in range(n_images):\n",
        "    # We use leave-one-out so train will be all but image i, val will be image i\n",
        "    print('validation using image_{:02d}.jpg'.format(i+1))\n",
        "    train_selection = image_index != float(i)\n",
        "    fold_x = xs[train_selection].numpy()\n",
        "    fold_y = (ys[train_selection] > 0).long().numpy()\n",
        "    val_x = xs[~train_selection].numpy()\n",
        "    val_y = (ys[~train_selection] > 0).long().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    for j, c in enumerate(cs):\n",
        "        print(\"training logisitic regression with C={:.2e}\".format(c))\n",
        "        clf = LogisticRegression(random_state=0, C=c, max_iter=10000).fit(fold_x, fold_y)\n",
        "        output = clf.predict_proba(val_x)\n",
        "        precision, recall, thresholds = precision_recall_curve(val_y, output[:, 1])\n",
        "        s = average_precision_score(val_y, output[:, 1])\n",
        "        scores[i, j] = s\n",
        "        plt.plot(recall, precision, label='C={:.1e} AP={:.1f}'.format(c, s*100))\n",
        "\n",
        "    plt.grid()\n",
        "    plt.xlabel('recall')\n",
        "    plt.title('image_{:02d}.jpg'.format(i+1))\n",
        "    plt.ylabel('precision')\n",
        "    plt.axis([0, 1, 0, 1])\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c484eb00",
      "metadata": {
        "id": "c484eb00"
      },
      "source": [
        "### Choosing the Best C\n",
        "Now, let's have a look at which value of C works best on average. To this end we will plot the average mAP across all validation images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27512758",
      "metadata": {
        "id": "27512758"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(3, 2), dpi=300)\n",
        "plt.rcParams.update({\n",
        "    \"xtick.labelsize\": 5,\n",
        "    \"ytick.labelsize\": 5,\n",
        "    \"axes.labelsize\": 5,\n",
        "})\n",
        "plt.plot(scores.mean(axis=0))\n",
        "plt.xticks(np.arange(len(cs)), [\"{:.0e}\".format(c) for c in cs])\n",
        "plt.xlabel('data fit C')\n",
        "plt.ylabel('average AP')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582a8985",
      "metadata": {
        "id": "582a8985"
      },
      "source": [
        "### Retraining with the optimal regularization\n",
        "Given the above, we seem to have a winner: C=0.1.\n",
        "Let's now train a model using this optimal data-fit value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "787d4f93",
      "metadata": {
        "id": "787d4f93"
      },
      "outputs": [],
      "source": [
        "clf = LogisticRegression(random_state=0, C=0.1, max_iter=100000, verbose=2).fit(xs.numpy(), (ys > 0).long().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdd5ec72",
      "metadata": {
        "id": "fdd5ec72"
      },
      "source": [
        "### Test Images and Inference\n",
        "\n",
        "We have a classifier, now it is time to test it! We will predict the probability of patch being foreground given an image, and then process it with a 3x3 median filter to smooth it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d17d189e",
      "metadata": {
        "id": "d17d189e"
      },
      "outputs": [],
      "source": [
        "test_image_fpath = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/foreground_segmentation/test_image.jpg\"\n",
        "\n",
        "def load_image_from_url(url: str) -> Image:\n",
        "    with urllib.request.urlopen(url) as f:\n",
        "        return Image.open(f).convert(\"RGB\")\n",
        "\n",
        "\n",
        "test_image = load_image_from_url(test_image_fpath)\n",
        "test_image_resized = resize_transform(test_image)\n",
        "test_image_normalized = TF.normalize(test_image_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
        "        feats = model.get_intermediate_layers(test_image_normalized.unsqueeze(0).cuda(), n=range(n_layers), reshape=True, norm=True)\n",
        "        x = feats[-1].squeeze().detach().cpu()\n",
        "        dim = x.shape[0]\n",
        "        x = x.view(dim, -1).permute(1, 0)\n",
        "\n",
        "h_patches, w_patches = [int(d / PATCH_SIZE) for d in test_image_resized.shape[1:]]\n",
        "\n",
        "fg_score = clf.predict_proba(x)[:, 1].reshape(h_patches, w_patches)\n",
        "fg_score_mf = torch.from_numpy(signal.medfilt2d(fg_score, kernel_size=3))\n",
        "\n",
        "plt.figure(figsize=(9, 3), dpi=300)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.axis('off')\n",
        "plt.imshow(test_image_resized.permute(1, 2, 0))\n",
        "plt.title('input image')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.axis('off')\n",
        "plt.imshow(fg_score)\n",
        "plt.title('foreground score')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.axis('off')\n",
        "plt.imshow(fg_score_mf)\n",
        "plt.title('+ median filter')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5ee6bce",
      "metadata": {
        "id": "c5ee6bce"
      },
      "source": [
        "### Saving the PyTorch Model for Future Use\n",
        "We are nearly done, let's just save a pickle with the classifier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a33e2980",
      "metadata": {
        "id": "a33e2980"
      },
      "outputs": [],
      "source": [
        "save_root = '.'\n",
        "model_path = os.path.join(save_root, \"fg_classifier.pkl\")\n",
        "with open(model_path, \"wb\") as f:\n",
        "  pickle.dump(clf, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX Model Export\n",
        "- Export DINOv3 Feature Extractor to ONNX\n",
        "- Export Classifier to ONNX\n",
        "- device = \"CPU\"\n",
        "- inputs are independent of PyTorch"
      ],
      "metadata": {
        "id": "WPlNGDh7JaSA"
      },
      "id": "WPlNGDh7JaSA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install ONNX library\n",
        "!pip install onnx\n",
        "# Install ONNX Runtime\n",
        "!pip install onnxruntime"
      ],
      "metadata": {
        "id": "flZQb4jWOEP_"
      },
      "id": "flZQb4jWOEP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import onnxruntime as ort"
      ],
      "metadata": {
        "id": "Apqz1uyzOKmT"
      },
      "id": "Apqz1uyzOKmT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Starting ONNX Model Export ---\")\n",
        "\n",
        "# --- Define PyTorch wrapper for DINOv3 Feature Extractor ---\n",
        "class DinoV3FeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self, model, n_layers):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features from the last layer, normalized, as done during training\n",
        "        features_list = self.model.get_intermediate_layers(x, n=range(self.n_layers), reshape=True, norm=True)\n",
        "        last_layer_features = features_list[-1]  # Shape: (B, C, H_patches, W_patches)\n",
        "\n",
        "        # Reshape for classifier: (B, C, H*W) -> (B, H*W, C)\n",
        "        B, C, H, W = last_layer_features.shape\n",
        "        features_reshaped = last_layer_features.view(B, C, -1)\n",
        "        features_permuted = features_reshaped.permute(0, 2, 1)\n",
        "        return features_permuted\n",
        "\n",
        "# --- Define PyTorch wrapper for Logistic Regression Classifier ---\n",
        "class LogisticRegressionONNX(torch.nn.Module):\n",
        "    def __init__(self, sklearn_classifier):\n",
        "        super().__init__()\n",
        "        # Extract weights and bias from the trained sklearn model\n",
        "        self.coeffs = torch.nn.Parameter(torch.from_numpy(sklearn_classifier.coef_).float())\n",
        "        self.intercept = torch.nn.Parameter(torch.from_numpy(sklearn_classifier.intercept_).float())\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Input 'x' has shape (B, Num_Patches, Channels)\n",
        "        # Apply linear transformation: x @ W.T + b\n",
        "        linear_output = torch.matmul(x, self.coeffs.T) + self.intercept\n",
        "        # Apply sigmoid to get foreground probability\n",
        "        probabilities = torch.sigmoid(linear_output)\n",
        "        # Squeeze the last dimension to get (B, Num_Patches)\n",
        "        return probabilities.squeeze(-1)\n",
        "\n",
        "# --- Export DINOv3 Feature Extractor to ONNX ---\n",
        "onnx_feature_extractor_path = \"dinov3_feature_extractor.onnx\"\n",
        "device = \"cpu\"\n",
        "onnx_exportable_dino = DinoV3FeatureExtractor(model, n_layers).to(device).eval()\n",
        "# Dummy input with dynamic axes for variable image sizes\n",
        "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE, device=device)\n",
        "\n",
        "print(f\"Exporting DINOv3 feature extractor to {onnx_feature_extractor_path}...\")\n",
        "torch.onnx.export(\n",
        "    onnx_exportable_dino,\n",
        "    dummy_input,\n",
        "    onnx_feature_extractor_path,\n",
        "    input_names=['input_image'],\n",
        "    output_names=['patch_features'],\n",
        "    dynamic_axes={\n",
        "        'input_image': {2: 'height', 3: 'width'},\n",
        "        'patch_features': {1: 'num_patches'}\n",
        "    },\n",
        "    opset_version=17\n",
        ")\n",
        "print(\"DINOv3 ONNX export complete!\")\n",
        "\n",
        "# --- Export Classifier to ONNX ---\n",
        "onnx_classifier_path = \"fg_classifier.onnx\"\n",
        "onnx_exportable_clf = LogisticRegressionONNX(clf).eval()\n",
        "# Dummy input matching the output of the feature extractor\n",
        "num_channels = xs.shape[1]\n",
        "dummy_features = torch.randn(1, (IMAGE_SIZE // PATCH_SIZE)**2, num_channels)\n",
        "\n",
        "print(f\"\\nExporting classifier to {onnx_classifier_path}...\")\n",
        "torch.onnx.export(\n",
        "    onnx_exportable_clf,\n",
        "    dummy_features,\n",
        "    onnx_classifier_path,\n",
        "    input_names=['patch_features'],\n",
        "    output_names=['probabilities'],\n",
        "    dynamic_axes={\n",
        "        'patch_features': {1: 'num_patches'},\n",
        "        'probabilities': {1: 'num_patches'}\n",
        "    },\n",
        "    opset_version=17\n",
        ")\n",
        "print(\"Classifier ONNX export complete!\")\n",
        "\n",
        "print(\"\\n--- Starting Part 3: PyTorch-Independent ONNX Inference ---\")\n",
        "\n",
        "\n",
        "\n",
        "# --- Define Pre-processing Functions (NumPy/Pillow only) ---\n",
        "def preprocess_image_numpy(img_pil: Image.Image, image_size: int, patch_size: int):\n",
        "    \"\"\"Resizes and normalizes an image using NumPy and Pillow.\"\"\"\n",
        "    # Resize image to be divisible by patch size\n",
        "    w, h = img_pil.size\n",
        "    h_patches = image_size // patch_size\n",
        "    w_patches = int((w * image_size) / (h * patch_size))\n",
        "\n",
        "    # Ensure width is even for some model architectures\n",
        "    if w_patches % 2 != 0:\n",
        "      w_patches -= 1\n",
        "\n",
        "    new_h = h_patches * patch_size\n",
        "    new_w = w_patches * patch_size\n",
        "    resized_img = img_pil.resize((new_w, new_h), Image.Resampling.BICUBIC)\n",
        "\n",
        "    # Convert to NumPy array and normalize\n",
        "    img_np = np.array(resized_img, dtype=np.float32) / 255.0\n",
        "    mean = np.array(IMAGENET_MEAN, dtype=np.float32)\n",
        "    std = np.array(IMAGENET_STD, dtype=np.float32)\n",
        "    normalized_img = (img_np - mean) / std\n",
        "\n",
        "    # Transpose from HWC to CHW and add batch dimension (BCHW)\n",
        "    input_tensor = normalized_img.transpose(2, 0, 1)[np.newaxis, :, :]\n",
        "\n",
        "    return input_tensor, (h_patches, w_patches), resized_img\n",
        "\n",
        "# --- Load Test Image ---\n",
        "test_image_fpath = \"https://dl.fbaipublicfiles.com/dinov3/notebooks/foreground_segmentation/test_image.jpg\"\n",
        "with urllib.request.urlopen(test_image_fpath) as f:\n",
        "    test_image = Image.open(f).convert(\"RGB\")\n",
        "\n",
        "# --- Preprocess Image ---\n",
        "input_tensor, patch_dims, resized_image_pil = preprocess_image_numpy(test_image, IMAGE_SIZE, PATCH_SIZE)\n",
        "h_patches, w_patches = patch_dims\n",
        "print(f\"Test image preprocessed. Patch grid size: {patch_dims}\")\n",
        "\n",
        "# --- Run Inference with ONNX Runtime ---\n",
        "# Create inference sessions\n",
        "print(\"Creating ONNX Runtime inference sessions...\")\n",
        "feature_extractor_session = ort.InferenceSession(onnx_feature_extractor_path)\n",
        "classifier_session = ort.InferenceSession(onnx_classifier_path)\n",
        "\n",
        "# Get patch features from DINOv3\n",
        "print(\"Running feature extraction...\")\n",
        "feature_inputs = {feature_extractor_session.get_inputs()[0].name: input_tensor}\n",
        "patch_features = feature_extractor_session.run(None, feature_inputs)[0]\n",
        "\n",
        "# Get foreground probabilities from classifier\n",
        "print(\"Running classification...\")\n",
        "classifier_inputs = {classifier_session.get_inputs()[0].name: patch_features}\n",
        "fg_probabilities = classifier_session.run(None, classifier_inputs)[0]\n",
        "\n",
        "# --- Post-process and Visualize Results ---\n",
        "# Reshape probabilities to match patch grid\n",
        "fg_score = fg_probabilities.reshape(h_patches, w_patches)\n",
        "\n",
        "# Apply median filter for smoothing\n",
        "fg_score_mf = signal.medfilt2d(fg_score, kernel_size=3)\n",
        "\n",
        "# Display the results\n",
        "print(\"Displaying final segmentation results...\")\n",
        "plt.figure(figsize=(12, 4), dpi=150)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(resized_image_pil)\n",
        "plt.title('Input Image')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(fg_score, cmap='viridis')\n",
        "plt.title('Foreground Score')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(fg_score_mf, cmap='viridis')\n",
        "plt.title('+ Median Filter')\n",
        "plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n--- Script finished successfully! ---\")"
      ],
      "metadata": {
        "id": "Xphy9gKM_0-r"
      },
      "id": "Xphy9gKM_0-r",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}