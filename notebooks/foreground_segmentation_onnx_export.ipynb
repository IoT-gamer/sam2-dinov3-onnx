{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoT-gamer/segment-anything-dinov3-onnx/blob/main/notebooks/foreground_segmentation_onnx_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9966b9cd",
      "metadata": {
        "id": "9966b9cd"
      },
      "source": [
        "# Training a Foreground Segmentation Tool with DINOv3 & Export Model in ONNX Format\n",
        "\n",
        "- train a foreground segmentation model using DINOv3 features.\n",
        "- export DINOv3 Feature Extractor to ONNX\n",
        "- export Logistic Regression Classifier to ONNX"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "U0M3Au81HbQS",
      "metadata": {
        "id": "U0M3Au81HbQS"
      },
      "source": [
        "## Acknoweldgements/References\n",
        "- [DINOv3 github repo](https://github.com/facebookresearch/dinov3)\n",
        "- [foreground_segmentation.ipynb](https://github.com/facebookresearch/dinov3/blob/main/notebooks/foreground_segmentation.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03c01a4-2fe0-4d61-b05c-9d3274bda2df",
      "metadata": {
        "id": "a03c01a4-2fe0-4d61-b05c-9d3274bda2df"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uncomment if storing model and/or images in google drive and using google colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "xoEw3pBeO5KD"
      },
      "id": "xoEw3pBeO5KD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx onnxruntime"
      ],
      "metadata": {
        "id": "JSabDi6D-q79"
      },
      "id": "JSabDi6D-q79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4_KnxlWnGZtf",
      "metadata": {
        "id": "4_KnxlWnGZtf"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "import io\n",
        "import os\n",
        "import pickle\n",
        "import tarfile\n",
        "import urllib\n",
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import torch\n",
        "import torchvision.transforms.functional as TF\n",
        "from tqdm import tqdm\n",
        "\n",
        "import onnx\n",
        "import onnxruntime as ort"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Loading\n",
        "- visit https://ai.meta.com/resources/models-and-libraries/dinov3-downloads/ to get weights URLs\n",
        "  - note: URLs may expire after a few days\n",
        "- two options:\n",
        "  1. download the weigths and store locally (recommended)\n",
        "  2. add a secret named `dinov3_vits16` and copy and past the dinov3_vits16 URL in the value."
      ],
      "metadata": {
        "id": "CQSI7OVV_MNJ"
      },
      "id": "CQSI7OVV_MNJ"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DINOV3_VITS = \"dinov3_vits16\"\n",
        "MODEL_NAME = MODEL_DINOV3_VITS\n",
        "\n",
        "DEVICE = \"cpu\" # Select cuda or cpu\n",
        "\n",
        "# Set the path to your local weights file. If `None`, it will download from the URL.\n",
        "# For example: WEIGHTS_LOCAL_PATH = \"/path/to/dinov3_vits16.pt\"\n",
        "\n",
        "# WEIGHTS_LOCAL_PATH = None\n",
        "WEIGHTS_LOCAL_PATH = \"/path/to/dinov3_vits16_pretrain_lvd1689m-08c60483.pth\"\n",
        "\n",
        "if WEIGHTS_LOCAL_PATH and os.path.exists(WEIGHTS_LOCAL_PATH):\n",
        "    print(f\"Loading weights from local path: {WEIGHTS_LOCAL_PATH}\")\n",
        "    weights_source = WEIGHTS_LOCAL_PATH\n",
        "else:\n",
        "    print(\"Loading weights from remote URL.\")\n",
        "    # Add a secret named `dinov3_vits16` in Colab with the URL as the value\n",
        "    try:\n",
        "        WEIGHTS_URL = userdata.get('dinov3_vits16')\n",
        "        weights_source = WEIGHTS_URL\n",
        "    except Exception as e:\n",
        "        print(f\"Could not retrieve weights URL from Colab secrets. Please set WEIGHTS_LOCAL_PATH. Error: {e}\")\n",
        "        weights_source = None # Will cause an error if not set\n",
        "\n",
        "# Load the model\n",
        "model = torch.hub.load(\n",
        "    repo_or_dir=\"facebookresearch/dinov3\",\n",
        "    model=MODEL_NAME,\n",
        "    source=\"github\",\n",
        "    weights=weights_source\n",
        ")\n",
        "\n",
        "if DEVICE == \"cuda\":\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n"
      ],
      "metadata": {
        "id": "ul5gai27_P6o"
      },
      "id": "ul5gai27_P6o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading\n",
        "- RGBA .png images with masks stored in alpha channels"
      ],
      "metadata": {
        "id": "sLfNx1abFXw7"
      },
      "id": "sLfNx1abFXw7"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_rgba_data(data_dir: str) -> tuple[list[Image.Image], list[Image.Image]]:\n",
        "    \"\"\"\n",
        "    Loads all RGBA .png images from a local directory.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Path to the directory containing .png files.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing two lists:\n",
        "        - A list of RGB images (PIL.Image.Image).\n",
        "        - A list of Alpha channel masks (PIL.Image.Image).\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # Use glob to find all .png files in the directory\n",
        "    image_paths = glob.glob(os.path.join(data_dir, \"*.png\"))\n",
        "    if not image_paths:\n",
        "        raise FileNotFoundError(f\"No .png files found in the directory: {data_dir}\")\n",
        "\n",
        "    print(f\"Found {len(image_paths)} .png files in {data_dir}\")\n",
        "\n",
        "    for image_path in tqdm(image_paths, desc=\"Loading local images\"):\n",
        "        with Image.open(image_path) as img:\n",
        "            if img.mode != 'RGBA':\n",
        "                print(f\"Warning: Image {Path(image_path).name} is not in RGBA mode. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Ensure data is loaded into memory before closing the file\n",
        "            img.load()\n",
        "\n",
        "            # Separate RGB and Alpha channels\n",
        "            rgb_image = img.convert(\"RGB\")\n",
        "            alpha_mask = img.split()[-1]\n",
        "\n",
        "            images.append(rgb_image)\n",
        "            masks.append(alpha_mask)\n",
        "\n",
        "    return images, masks"
      ],
      "metadata": {
        "id": "0wyGx_kXFUS1"
      },
      "id": "0wyGx_kXFUS1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Source"
      ],
      "metadata": {
        "id": "AUx1bLVL6zBx"
      },
      "id": "AUx1bLVL6zBx"
    },
    {
      "cell_type": "code",
      "source": [
        "LOCAL_DATA_PATH = \"/path/to/your/rgba_png_folder\"\n",
        "\n",
        "images, labels = load_rgba_data(LOCAL_DATA_PATH)\n",
        "\n",
        "n_images = len(images)\n",
        "print(f\"Loaded {n_images} images and labels.\")"
      ],
      "metadata": {
        "id": "aaoUqFHkFyHb"
      },
      "id": "aaoUqFHkFyHb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize an Example"
      ],
      "metadata": {
        "id": "s2969DR4UAok"
      },
      "id": "s2969DR4UAok"
    },
    {
      "cell_type": "code",
      "source": [
        "if n_images > 0:\n",
        "    data_index = 0\n",
        "    print(f\"Showing image / mask at index {data_index}:\")\n",
        "    image = images[data_index]\n",
        "\n",
        "    mask = labels[data_index]\n",
        "    foreground = Image.composite(image, mask, mask)\n",
        "\n",
        "    plt.figure(figsize=(12, 4), dpi=150)\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(\"Image\")\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.imshow(mask, cmap='gray')\n",
        "    plt.title(\"Mask (Alpha Channel)\")\n",
        "    plt.axis('off')\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.imshow(foreground)\n",
        "    plt.title(\"Foreground\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "IsnNkj27UJmN"
      },
      "id": "IsnNkj27UJmN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building Per-Patch Label Map\n",
        "Since our models run with a patch size of 16, we have to quantize the ground truth to a 16x16 pixels grid. To achieve this, we define:\n",
        "- the resize transform to resize an image such that it aligns well with the 16x16 grid;\n",
        "- a uniform 16x16 conv layer as a [box blur filter](https://en.wikipedia.org/wiki/Box_blur) with stride equal to the patch size."
      ],
      "metadata": {
        "id": "qolZ-7H1UN7o"
      },
      "id": "qolZ-7H1UN7o"
    },
    {
      "cell_type": "code",
      "source": [
        "PATCH_SIZE = 16\n",
        "IMAGE_SIZE = 768\n",
        "\n",
        "patch_quant_filter = torch.nn.Conv2d(1, 1, PATCH_SIZE, stride=PATCH_SIZE, bias=False)\n",
        "patch_quant_filter.weight.data.fill_(1.0 / (PATCH_SIZE * PATCH_SIZE))\n",
        "\n",
        "def resize_transform(\n",
        "    img_or_mask: Image,\n",
        "    image_size: int = IMAGE_SIZE,\n",
        "    patch_size: int = PATCH_SIZE,\n",
        ") -> torch.Tensor:\n",
        "    w, h = img_or_mask.size\n",
        "    h_patches = int(image_size / patch_size)\n",
        "    w_patches = int((w * image_size) / (h * patch_size))\n",
        "    target_size = (h_patches * patch_size, w_patches * patch_size)\n",
        "    return TF.to_tensor(TF.resize(img_or_mask, target_size))"
      ],
      "metadata": {
        "id": "Y1gR-Gf2UUtZ"
      },
      "id": "Y1gR-Gf2UUtZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting Features and Labels for All Images"
      ],
      "metadata": {
        "id": "ae7sZOr8UdHb"
      },
      "id": "ae7sZOr8UdHb"
    },
    {
      "cell_type": "code",
      "source": [
        "xs = []\n",
        "ys = []\n",
        "image_index = []\n",
        "\n",
        "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
        "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
        "\n",
        "MODEL_TO_NUM_LAYERS = {MODEL_DINOV3_VITS: 12}\n",
        "n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast(device_type=device.type, dtype=torch.float32):\n",
        "        for i in tqdm(range(n_images), desc=\"Processing images\"):\n",
        "            mask_i = labels[i]\n",
        "            if mask_i.mode == 'RGBA':\n",
        "                mask_i = mask_i.split()[-1]\n",
        "\n",
        "            mask_i_resized = resize_transform(mask_i)\n",
        "            mask_i_quantized = patch_quant_filter(mask_i_resized).squeeze().view(-1).detach().cpu()\n",
        "            ys.append(mask_i_quantized)\n",
        "\n",
        "            # Loading the image data (already in RGB)\n",
        "            image_i = images[i]\n",
        "            image_i_resized = resize_transform(image_i)\n",
        "            image_i_normalized = TF.normalize(image_i_resized, mean=IMAGENET_MEAN, std=IMAGENET_STD)\n",
        "            if device == torch.device('cuda'):\n",
        "                image_i_tensor = image_i_normalized.unsqueeze(0).cuda()\n",
        "            else:\n",
        "                image_i_tensor = image_i_normalized.unsqueeze(0)\n",
        "            # image_i_tensor = image_i_normalized.unsqueeze(0).cuda()\n",
        "\n",
        "            feats = model.get_intermediate_layers(image_i_tensor, n=range(n_layers), reshape=True, norm=True)\n",
        "            dim = feats[-1].shape[1]\n",
        "            xs.append(feats[-1].squeeze().view(dim, -1).permute(1,0).detach().cpu())\n",
        "\n",
        "            image_index.append(i * torch.ones(ys[-1].shape))\n",
        "\n",
        "\n",
        "# Concatenate all lists into torch tensors\n",
        "xs = torch.cat(xs)\n",
        "ys = torch.cat(ys)\n",
        "image_index = torch.cat(image_index)\n",
        "\n",
        "# Keep only patches with clear positive or negative labels\n",
        "idx = (ys < 0.01) | (ys > 0.99)\n",
        "xs = xs[idx]\n",
        "ys = ys[idx]\n",
        "image_index = image_index[idx]\n",
        "\n",
        "print(\"Design matrix size: \", xs.shape)\n",
        "print(\"Label matrix size: \", ys.shape)"
      ],
      "metadata": {
        "id": "NS7bG7fzUsUU"
      },
      "id": "NS7bG7fzUsUU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Classifier and Model Selection\n",
        "- We will do leave-one-out, and consecutively exclude each image as a validation set. We'll try 8 values of C ranging from 1e-7 to 1e-0."
      ],
      "metadata": {
        "id": "BIPHjrP5Uu8D"
      },
      "id": "BIPHjrP5Uu8D"
    },
    {
      "cell_type": "code",
      "source": [
        "cs = np.logspace(-7, 0, 8)\n",
        "scores = np.zeros((n_images, len(cs)))\n",
        "\n",
        "for i in range(n_images):\n",
        "    print(f'Validation using image {i+1}/{n_images}')\n",
        "    train_selection = image_index != float(i)\n",
        "    fold_x = xs[train_selection].numpy()\n",
        "    fold_y = (ys[train_selection] > 0).long().numpy()\n",
        "    val_x = xs[~train_selection].numpy()\n",
        "    val_y = (ys[~train_selection] > 0).long().numpy()\n",
        "\n",
        "    for j, c in enumerate(cs):\n",
        "        clf = LogisticRegression(random_state=0, C=c, max_iter=10000, n_jobs=-1).fit(fold_x, fold_y)\n",
        "        output = clf.predict_proba(val_x)\n",
        "        s = average_precision_score(val_y, output[:, 1])\n",
        "        scores[i, j] = s"
      ],
      "metadata": {
        "id": "EcZcaxYqU7RO"
      },
      "id": "EcZcaxYqU7RO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Choosing the Best C"
      ],
      "metadata": {
        "id": "8RTtg_Vfidkw"
      },
      "id": "8RTtg_Vfidkw"
    },
    {
      "cell_type": "code",
      "source": [
        "best_c_index = scores.mean(axis=0).argmax()\n",
        "best_c = cs[best_c_index]\n",
        "print(f\"\\nBest C value found: {best_c:.2e}\")"
      ],
      "metadata": {
        "id": "-jeyMWjjijCO"
      },
      "id": "-jeyMWjjijCO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retraining with the optimal regularization"
      ],
      "metadata": {
        "id": "FbMkQ9nein3i"
      },
      "id": "FbMkQ9nein3i"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Retraining classifier with the optimal C on all data...\")\n",
        "clf = LogisticRegression(random_state=0, C=best_c, max_iter=100000, verbose=0, n_jobs=-1).fit(xs.numpy(), (ys > 0).long().numpy())\n",
        "print(\"Final classifier trained.\")"
      ],
      "metadata": {
        "id": "D2RwIk-XilN5"
      },
      "id": "D2RwIk-XilN5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the PyTorch Model"
      ],
      "metadata": {
        "id": "tsyAAqVUixJ9"
      },
      "id": "tsyAAqVUixJ9"
    },
    {
      "cell_type": "code",
      "source": [
        "save_root = '.'\n",
        "model_path = os.path.join(save_root, \"fg_classifier.pkl\")\n",
        "with open(model_path, \"wb\") as f:\n",
        "  pickle.dump(clf, f)\n",
        "print(f\"Classifier saved to {model_path}\")"
      ],
      "metadata": {
        "id": "NOvR9tIfi1Ha"
      },
      "id": "NOvR9tIfi1Ha",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ONNX Model Export"
      ],
      "metadata": {
        "id": "fPumD1EmdaeT"
      },
      "id": "fPumD1EmdaeT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Wrappers"
      ],
      "metadata": {
        "id": "HCGnDF4B8B32"
      },
      "id": "HCGnDF4B8B32"
    },
    {
      "cell_type": "code",
      "source": [
        "class DinoV3FeatureExtractor(torch.nn.Module):\n",
        "    def __init__(self, model, n_layers):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        features_list = self.model.get_intermediate_layers(x, n=range(self.n_layers), reshape=True, norm=True)\n",
        "        last_layer_features = features_list[-1]\n",
        "        B, C, H, W = last_layer_features.shape\n",
        "        return last_layer_features.view(B, C, -1).permute(0, 2, 1)\n",
        "\n",
        "class LogisticRegressionONNX(torch.nn.Module):\n",
        "    def __init__(self, sklearn_classifier):\n",
        "        super().__init__()\n",
        "        self.coeffs = torch.nn.Parameter(torch.from_numpy(sklearn_classifier.coef_).float())\n",
        "        self.intercept = torch.nn.Parameter(torch.from_numpy(sklearn_classifier.intercept_).float())\n",
        "\n",
        "    def forward(self, x):\n",
        "        linear_output = torch.matmul(x, self.coeffs.T) + self.intercept\n",
        "        probabilities = torch.sigmoid(linear_output)\n",
        "        return probabilities.squeeze(-1)\n"
      ],
      "metadata": {
        "id": "6i54wYygdd9N"
      },
      "id": "6i54wYygdd9N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export DINOv3 Feature Extractor"
      ],
      "metadata": {
        "id": "GmyboSct8J-8"
      },
      "id": "GmyboSct8J-8"
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_feature_extractor_path = \"dinov3_feature_extractor.onnx\"\n",
        "device = \"cpu\"\n",
        "onnx_exportable_dino = DinoV3FeatureExtractor(model, n_layers).to(device).eval()\n",
        "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE, device=device)\n",
        "\n",
        "print(f\"Exporting DINOv3 feature extractor to {onnx_feature_extractor_path}...\")\n",
        "torch.onnx.export(\n",
        "    onnx_exportable_dino, dummy_input, onnx_feature_extractor_path,\n",
        "    input_names=['input_image'], output_names=['patch_features'],\n",
        "    dynamic_axes={'input_image': {2: 'height', 3: 'width'}, 'patch_features': {1: 'num_patches'}},\n",
        "    opset_version=17\n",
        ")\n",
        "print(\"DINOv3 ONNX export complete!\")"
      ],
      "metadata": {
        "id": "8GeMYxx_8Vnr"
      },
      "id": "8GeMYxx_8Vnr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Classifier"
      ],
      "metadata": {
        "id": "L_L2MkRD8Z_3"
      },
      "id": "L_L2MkRD8Z_3"
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_classifier_path = \"fg_classifier.onnx\"\n",
        "onnx_exportable_clf = LogisticRegressionONNX(clf).eval()\n",
        "num_channels = xs.shape[1]\n",
        "dummy_features = torch.randn(1, (IMAGE_SIZE // PATCH_SIZE)**2, num_channels)\n",
        "\n",
        "print(f\"\\nExporting classifier to {onnx_classifier_path}...\")\n",
        "torch.onnx.export(\n",
        "    onnx_exportable_clf, dummy_features, onnx_classifier_path,\n",
        "    input_names=['patch_features'], output_names=['probabilities'],\n",
        "    dynamic_axes={'patch_features': {1: 'num_patches'}, 'probabilities': {1: 'num_patches'}},\n",
        "    opset_version=17\n",
        ")\n",
        "print(\"Classifier ONNX export complete!\")"
      ],
      "metadata": {
        "id": "xar-AKEK8ZQY"
      },
      "id": "xar-AKEK8ZQY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PyTorch-Independent ONNX Inference"
      ],
      "metadata": {
        "id": "xc7zQ7xJ9g4o"
      },
      "id": "xc7zQ7xJ9g4o"
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image_numpy(img_pil: Image.Image, image_size: int, patch_size: int):\n",
        "    \"\"\"Resizes and normalizes an image using NumPy and Pillow.\"\"\"\n",
        "    w, h = img_pil.size\n",
        "    h_patches = image_size // patch_size\n",
        "    w_patches = int((w * image_size) / (h * patch_size))\n",
        "    if w_patches % 2 != 0: w_patches -= 1\n",
        "\n",
        "    new_h, new_w = h_patches * patch_size, w_patches * patch_size\n",
        "    resized_img = img_pil.resize((new_w, new_h), Image.Resampling.BICUBIC)\n",
        "\n",
        "    img_np = np.array(resized_img, dtype=np.float32) / 255.0\n",
        "    mean = np.array(IMAGENET_MEAN, dtype=np.float32)\n",
        "    std = np.array(IMAGENET_STD, dtype=np.float32)\n",
        "    normalized_img = (img_np - mean) / std\n",
        "\n",
        "    input_tensor = normalized_img.transpose(2, 0, 1)[np.newaxis, :, :]\n",
        "    return input_tensor, (h_patches, w_patches), resized_img"
      ],
      "metadata": {
        "id": "KDQfjRDh9wIi"
      },
      "id": "KDQfjRDh9wIi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Test Image"
      ],
      "metadata": {
        "id": "1NTK_so89_L5"
      },
      "id": "1NTK_so89_L5"
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_fpath = \"/path/to/your/test_rgba.png\"\n",
        "test_image = Image.open(test_image_fpath).convert('RGB')"
      ],
      "metadata": {
        "id": "PDCzAmYP9-pp"
      },
      "id": "PDCzAmYP9-pp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess and Run Inference"
      ],
      "metadata": {
        "id": "5hHXH0Ol-5ys"
      },
      "id": "5hHXH0Ol-5ys"
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor, patch_dims, resized_image_pil = preprocess_image_numpy(test_image, IMAGE_SIZE, PATCH_SIZE)\n",
        "h_patches, w_patches = patch_dims\n",
        "\n",
        "feature_extractor_session = ort.InferenceSession(onnx_feature_extractor_path)\n",
        "classifier_session = ort.InferenceSession(onnx_classifier_path)\n",
        "\n",
        "feature_inputs = {feature_extractor_session.get_inputs()[0].name: input_tensor}\n",
        "patch_features = feature_extractor_session.run(None, feature_inputs)[0]\n",
        "\n",
        "classifier_inputs = {classifier_session.get_inputs()[0].name: patch_features}\n",
        "fg_probabilities = classifier_session.run(None, classifier_inputs)[0]"
      ],
      "metadata": {
        "id": "J_1qbXL_-6Lc"
      },
      "id": "J_1qbXL_-6Lc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Post-process and Visualize"
      ],
      "metadata": {
        "id": "cNBD7Jm9_Gjp"
      },
      "id": "cNBD7Jm9_Gjp"
    },
    {
      "cell_type": "code",
      "source": [
        "fg_score = fg_probabilities.reshape(h_patches, w_patches)\n",
        "fg_score_mf = signal.medfilt2d(fg_score, kernel_size=3)\n",
        "\n",
        "print(\"Displaying final segmentation results...\")\n",
        "plt.figure(figsize=(12, 4), dpi=150)\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.imshow(resized_image_pil)\n",
        "plt.title('Input Image'); plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.imshow(fg_score, cmap='viridis')\n",
        "plt.title('Foreground Score'); plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.imshow(fg_score_mf, cmap='viridis')\n",
        "plt.title('+ Median Filter'); plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RGR1r_fc_Gu4"
      },
      "id": "RGR1r_fc_Gu4",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}