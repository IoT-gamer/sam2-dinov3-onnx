{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN+CVjR2KonBx7v8NhGtdPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IoT-gamer/sam2-dinov3-onnx/blob/main/notebooks/edgetam_onnx_export.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## EdgeTAM ONNX Conversion\n",
        "- only exporting encoder and decoder\n",
        "  - memory attention is challenging to export\n",
        "- inputs/outputs to onnx models use NumPy/OpenCV\n",
        "  - PyTorch-independent\n",
        "\n",
        "- image size is fixed at 1024x1024\n",
        "- device = CPU or GPU"
      ],
      "metadata": {
        "id": "WLPztHvXlY7a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References/Acknowledgments\n",
        "- [Official PyTorch implementation of \"EdgeTAM: On-Device Track Anything Model\"](https://github.com/facebookresearch/EdgeTAM)"
      ],
      "metadata": {
        "id": "OwfFDrynllaS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup"
      ],
      "metadata": {
        "id": "0h7OmM8alO0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clone Official EdgeTAM Repo"
      ],
      "metadata": {
        "id": "-CDHX9q4maHH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jWb_q5wjgLF"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/EdgeTAM.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd EdgeTAM"
      ],
      "metadata": {
        "id": "J9W0tbPDofzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup Device"
      ],
      "metadata": {
        "id": "alsoGgi8WNfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "82a49zvzWRMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install Dependencies"
      ],
      "metadata": {
        "id": "A2n7EBx-mAjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env SAM2_BUILD_CUDA=0\n",
        "!pip install -e ."
      ],
      "metadata": {
        "id": "583_5cdVpB37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx\n",
        "if device == \"cuda\":\n",
        "  !pip install onnxruntime-gpu\n",
        "else:\n",
        "  !pip install onnxruntime"
      ],
      "metadata": {
        "id": "XFXOvsed0fgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Constants"
      ],
      "metadata": {
        "id": "N6YOy1CxTeWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EDGETAM_ENCODER_PATH = \"edgetam_encoder.onnx\"\n",
        "EDGETAM_DECODER_PATH = \"edgetam_decoder.onnx\"\n",
        "EDGETAM_INPUT_SIZE = 1024 # currenty fixed"
      ],
      "metadata": {
        "id": "VqKMqPGvABIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Dependencies"
      ],
      "metadata": {
        "id": "lQyGKwaVTppU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from typing import Any\n",
        "import numpy as np\n",
        "import cv2\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from typing import Tuple\n",
        "from sam2.build_sam import build_sam2"
      ],
      "metadata": {
        "id": "_C77JR_hTwxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrapper Classes and Export Function"
      ],
      "metadata": {
        "id": "FeeGqpdrmFXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define Model Wrapper Classes\n",
        "class SAM2ImageEncoder(nn.Module):\n",
        "    def __init__(self, sam_model: Any):\n",
        "        super().__init__()\n",
        "        self.image_encoder = sam_model.image_encoder\n",
        "        self.sam_mask_decoder = sam_model.sam_mask_decoder\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        backbone_out = self.image_encoder(x)\n",
        "        backbone_out[\"backbone_fpn\"][0] = self.sam_mask_decoder.conv_s0(backbone_out[\"backbone_fpn\"][0])\n",
        "        backbone_out[\"backbone_fpn\"][1] = self.sam_mask_decoder.conv_s1(backbone_out[\"backbone_fpn\"][1])\n",
        "        return backbone_out[\"backbone_fpn\"][-1], backbone_out[\"backbone_fpn\"][-3], backbone_out[\"backbone_fpn\"][-2], backbone_out[\"vision_pos_enc\"][-1]\n",
        "\n",
        "class SAM2ImageDecoder(nn.Module):\n",
        "    def __init__(self, sam_model: Any, multimask_output: bool):\n",
        "        super().__init__()\n",
        "        self.mask_decoder = sam_model.sam_mask_decoder\n",
        "        self.prompt_encoder = sam_model.sam_prompt_encoder\n",
        "        self.multimask_output = multimask_output\n",
        "        self.image_size = sam_model.image_size\n",
        "    def forward(self, image_embed, high_res_feats_0, high_res_feats_1, point_coords, point_labels, mask_input, has_mask_input):\n",
        "        sparse_embedding = self._embed_points(point_coords, point_labels)\n",
        "        dense_embedding = self._embed_masks(mask_input, has_mask_input)\n",
        "        masks, iou_predictions, _, _ = self.mask_decoder.predict_masks(\n",
        "            image_embeddings=image_embed, image_pe=self.prompt_encoder.get_dense_pe(),\n",
        "            sparse_prompt_embeddings=sparse_embedding, dense_prompt_embeddings=dense_embedding,\n",
        "            repeat_image=False,\n",
        "            high_res_features=[high_res_feats_0, high_res_feats_1]\n",
        "        )\n",
        "        if self.multimask_output:\n",
        "            masks = masks[:, 1:, :, :]\n",
        "            iou_predictions = iou_predictions[:, 1:]\n",
        "        return masks, iou_predictions\n",
        "    def _embed_points(self, point_coords, point_labels):\n",
        "        point_coords = torch.cat([point_coords + 0.5, torch.zeros_like(point_coords[:, :1])], dim=1)\n",
        "        point_labels = torch.cat([point_labels, -torch.ones_like(point_labels[:, :1])], dim=1)\n",
        "        point_coords[..., 0] /= self.image_size\n",
        "        point_coords[..., 1] /= self.image_size\n",
        "        point_embedding = self.prompt_encoder.pe_layer._pe_encoding(point_coords)\n",
        "        point_labels = point_labels.unsqueeze(-1).expand_as(point_embedding)\n",
        "        point_embedding = point_embedding * (point_labels != -1)\n",
        "        point_embedding += self.prompt_encoder.not_a_point_embed.weight * (point_labels == -1)\n",
        "        for i in range(self.prompt_encoder.num_point_embeddings):\n",
        "            point_embedding += self.prompt_encoder.point_embeddings[i].weight * (point_labels == i)\n",
        "        return point_embedding\n",
        "    def _embed_masks(self, input_mask, has_mask_input):\n",
        "        mask_embedding = has_mask_input * self.prompt_encoder.mask_downscaling(input_mask)\n",
        "        mask_embedding += (1 - has_mask_input) * self.prompt_encoder.no_mask_embed.weight.reshape(1, -1, 1, 1)\n",
        "        return mask_embedding\n",
        "\n",
        "# Conversion Function with dummy inputs\n",
        "def convert_all_models_to_onnx(model_cfg, checkpoint_path, input_size, device, multimask_output):\n",
        "    # Build the model\n",
        "    sam2_model = build_sam2(model_cfg, checkpoint_path, device=device)\n",
        "    print(\"âœ… Model and checkpoint loaded successfully.\")\n",
        "\n",
        "    # Convert the model to ONNX\n",
        "    print(\"\\n[0/2] Converting EdgeTAM...\")\n",
        "\n",
        "    # Convert encoder\n",
        "    sam2_encoder = SAM2ImageEncoder(sam2_model).to(device).eval()\n",
        "    encoder_dummy_input = torch.randn(1, 3, input_size, input_size, device=device)\n",
        "    print(\"\\n[1/2] Converting Encoder...\")\n",
        "    torch.onnx.export(sam2_encoder, encoder_dummy_input, EDGETAM_ENCODER_PATH, opset_version=17,\n",
        "                      input_names=['image'], output_names=['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'vision_pos_enc'], dynamo=False)\n",
        "    print(\"âœ… Encoder exported.\")\n",
        "\n",
        "   # Convert decoder\n",
        "    sam2_decoder = SAM2ImageDecoder(sam2_model, multimask_output=multimask_output).to(device).eval()\n",
        "    image_embed, high_res_0, high_res_1, _ = sam2_encoder(encoder_dummy_input)\n",
        "    decoder_dummy_inputs = (\n",
        "        image_embed,\n",
        "        high_res_0,\n",
        "        high_res_1,\n",
        "        torch.randint(0, input_size, (1, 1, 2), dtype=torch.float, device=device),\n",
        "        torch.randint(0, 1, (1, 1), dtype=torch.float, device=device),\n",
        "        torch.randn(1, 1, 256, 256, device=device),\n",
        "        torch.tensor([1], dtype=torch.float, device=device)\n",
        "    )\n",
        "    print(\"\\n[2/2] Converting Decoder...\")\n",
        "    torch.onnx.export(sam2_decoder, decoder_dummy_inputs, EDGETAM_DECODER_PATH, opset_version=17,\n",
        "                      input_names=['image_embed', 'high_res_feats_0', 'high_res_feats_1', 'point_coords', 'point_labels', 'mask_input', 'has_mask_input'],\n",
        "                      output_names=['low_res_masks', 'iou_predictions'], dynamic_axes={'point_coords': {1: 'num_points'}, 'point_labels': {1: 'num_points'}}, dynamo=False)\n",
        "    print(\"âœ… Decoder exported.\")\n",
        "\n",
        "    print(\"\\nðŸŽ‰ All models converted successfully!\")\n"
      ],
      "metadata": {
        "id": "RMiRoHQblhbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export models"
      ],
      "metadata": {
        "id": "CJZhv9TmyuCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "convert_all_models_to_onnx(\n",
        "    model_cfg=\"configs/edgetam.yaml\",\n",
        "    checkpoint_path=\"checkpoints/edgetam.pt\",\n",
        "    input_size=EDGETAM_INPUT_SIZE,\n",
        "    device = device,\n",
        "    multimask_output=True\n",
        ")"
      ],
      "metadata": {
        "id": "ZXrIYtJRyriA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Static Image Test\n",
        "- predict mask from single point"
      ],
      "metadata": {
        "id": "xlYRIa4Y647A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_to_rgb(mask: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Converts a binary segmentation mask to a colorized RGB image.\"\"\"\n",
        "    # This creates a red overlay for the mask\n",
        "    rgb_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
        "    rgb_mask[mask == 1] = [0, 255, 0] # Green for the segmented object\n",
        "    return rgb_mask\n",
        "\n",
        "def preprocess_image(image_array: np.ndarray, input_size: int = 1024) -> np.ndarray:\n",
        "    \"\"\"Resizes, pads, and normalizes an image for EdgeTAM ONNX model inference.\"\"\"\n",
        "    orig_height, orig_width, _ = image_array.shape\n",
        "    resized_width, resized_height = input_size, input_size\n",
        "\n",
        "    input_array_resized = cv2.resize(image_array, (resized_width, resized_height), interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "    # Normalize with ImageNet stats\n",
        "    mean = np.array([123.675, 116.28, 103.53])\n",
        "    std = np.array([58.395, 57.12, 57.375])\n",
        "    input_tensor = (input_array_resized - mean) / std\n",
        "\n",
        "    # Transpose to CHW format and add batch dimension\n",
        "    input_tensor = input_tensor.transpose(2, 0, 1)[None, :, :, :].astype(np.float32)\n",
        "    return input_tensor\n",
        "\n",
        "def preprocess_point(\n",
        "    point: np.ndarray,\n",
        "    label: np.ndarray,\n",
        "    orig_size: Tuple[int, int],\n",
        "    resized_size: Tuple[int, int]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Preprocesses a point for EdgeTAM ONNX model inference.\"\"\"\n",
        "    orig_height, orig_width = orig_size\n",
        "    resized_height, resized_width = resized_size\n",
        "\n",
        "    onnx_coord = np.concatenate([point, np.array([[0.0, 0.0]])], axis=0)[None, :, :].astype(np.float32)\n",
        "    onnx_label = np.concatenate([label, np.array([-1])])[None, :].astype(np.float32)\n",
        "\n",
        "    # Scale coordinates to the resized image dimensions\n",
        "    onnx_coord[..., 0] = onnx_coord[..., 0] * (resized_width / orig_width)\n",
        "    onnx_coord[..., 1] = onnx_coord[..., 1] * (resized_height / orig_height)\n",
        "    return onnx_coord, onnx_label\n",
        "\n",
        "def run_inference(\n",
        "    encoder_session: ort.InferenceSession,\n",
        "    decoder_session: ort.InferenceSession,\n",
        "    image_tensor: np.ndarray,\n",
        "    point_coords: np.ndarray,\n",
        "    point_labels: np.ndarray,\n",
        "    original_size: Tuple[int, int]\n",
        ") -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Runs EdgeTAM inference and handles mask upscaling.\"\"\"\n",
        "    # Encoder inference\n",
        "    encoder_outputs = encoder_session.run(None, {'image': image_tensor})\n",
        "    image_embed, high_res_feats_0, high_res_feats_1, _ = encoder_outputs\n",
        "\n",
        "    # Decoder inference\n",
        "    onnx_mask_input = np.zeros((1, 1, 256, 256), dtype=np.float32)\n",
        "    onnx_has_mask_input = np.zeros(1, dtype=np.float32)\n",
        "\n",
        "    decoder_outputs = decoder_session.run(None, {\n",
        "        'image_embed': image_embed, 'high_res_feats_0': high_res_feats_0,\n",
        "        'high_res_feats_1': high_res_feats_1, \"point_coords\": point_coords,\n",
        "        \"point_labels\": point_labels, \"mask_input\": onnx_mask_input,\n",
        "        \"has_mask_input\": onnx_has_mask_input,\n",
        "    })\n",
        "    low_res_masks, iou_predictions = decoder_outputs\n",
        "\n",
        "    # Post-processing: Select the best mask and resize it\n",
        "    best_mask_idx = np.argmax(iou_predictions[0])\n",
        "    selected_low_res_mask = low_res_masks[0, best_mask_idx, :, :]\n",
        "\n",
        "    # Use OpenCV to resize the mask to the original image's dimensions\n",
        "    resized_mask = cv2.resize(\n",
        "        selected_low_res_mask,\n",
        "        (original_size[1], original_size[0]), # cv2 expects (width, height)\n",
        "        interpolation=cv2.INTER_LINEAR\n",
        "    )\n",
        "    return resized_mask, iou_predictions\n",
        "\n",
        "def overlay_transparent_mask(image_bgr, mask_rgb, binary_mask):\n",
        "    \"\"\"\n",
        "    Overlays a mask with true transparency onto a BGR image.\n",
        "\n",
        "    Args:\n",
        "        image_bgr: The background BGR image (3 channels).\n",
        "        mask_rgb: The mask image in RGB format (3 channels).\n",
        "        binary_mask: The binary mask to use for transparency (1 channel, 0 or 1).\n",
        "\n",
        "    Returns:\n",
        "        The resulting BGR image with the transparent mask overlayed.\n",
        "    \"\"\"\n",
        "    # Create an alpha channel from the binary mask.\n",
        "    alpha = (binary_mask * 255).astype(image_bgr.dtype)\n",
        "\n",
        "    # Create a 4-channel mask (RGB + Alpha).\n",
        "    # We need to make sure the mask_rgb and alpha have the same dimensions for stacking.\n",
        "    alpha_reshaped = cv2.merge([alpha, alpha, alpha])\n",
        "\n",
        "    # Invert the alpha channel for the background.\n",
        "    alpha_inv = cv2.bitwise_not(alpha)\n",
        "\n",
        "    # Extract the parts of the background and mask.\n",
        "    masked_background = cv2.bitwise_and(image_bgr, image_bgr, mask=alpha_inv)\n",
        "    masked_overlay = cv2.bitwise_and(mask_rgb, mask_rgb, mask=alpha)\n",
        "\n",
        "    # Combine the two parts.\n",
        "    overlayed_frame = cv2.add(masked_background, masked_overlay)\n",
        "\n",
        "    return overlayed_frame\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def overlay_point(image_bgr, point_coords, color, radius, thickness=-1):\n",
        "    \"\"\"\n",
        "    Overlays a point (as a filled circle) onto an image.\n",
        "\n",
        "    Args:\n",
        "        image_bgr (np.array): The background BGR image.\n",
        "        point_coords (np.array): The coordinates of the point to draw, e.g., np.array([[500, 375]]).\n",
        "        color (tuple): The BGR color for the point, e.g., (0, 0, 255) for red.\n",
        "        radius (int): The radius of the circle representing the point.\n",
        "        thickness (int): The thickness of the circle. Use -1 for a filled circle.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The image with the point overlayed.\n",
        "    \"\"\"\n",
        "    # Create a copy of the image to avoid modifying the original\n",
        "    overlayed_frame = image_bgr.copy()\n",
        "\n",
        "    # Get the coordinates from the numpy array\n",
        "    x, y = point_coords[0][0], point_coords[0][1]\n",
        "\n",
        "    # Draw the circle\n",
        "    cv2.circle(overlayed_frame, (x, y), radius, color, thickness)\n",
        "\n",
        "    return overlayed_frame\n"
      ],
      "metadata": {
        "id": "lN_xfynA8sb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = 'notebooks/images/truck.jpg'\n",
        "input_point = np.array([[500, 375]])\n",
        "input_label = np.array([1])\n",
        "\n",
        "image_bgr = cv2.imread(image_path)\n",
        "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
        "height, width, _ = image_rgb.shape\n",
        "orig_size = (height, width)\n",
        "input_tensor = preprocess_image(image_rgb, EDGETAM_INPUT_SIZE)\n",
        "onnx_coord, onnx_label = preprocess_point(\n",
        "    input_point, input_label, orig_size, (EDGETAM_INPUT_SIZE, EDGETAM_INPUT_SIZE)\n",
        ")\n",
        "\n",
        "edgetam_encoder_session = ort.InferenceSession(\n",
        "    EDGETAM_ENCODER_PATH,\n",
        "    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        ")\n",
        "edgetam_decoder_session = ort.InferenceSession(\n",
        "    EDGETAM_DECODER_PATH,\n",
        "    providers=[\"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        ")\n",
        "mask, _ = run_inference(\n",
        "    edgetam_encoder_session, edgetam_decoder_session,\n",
        "    input_tensor, onnx_coord, onnx_label, orig_size\n",
        ")\n",
        "binary_mask = (mask > 0).astype('uint8')\n",
        "mask_rgb = mask_to_rgb(binary_mask)\n",
        "overlay_image = overlay_transparent_mask(image_bgr, mask_rgb, binary_mask)\n",
        "final_image_with_point = overlay_point(overlay_image, input_point, (0, 0, 255), 10, -1)\n",
        "\n",
        "try:\n",
        "  from google.colab.patches import cv2_imshow\n",
        "  cv2_imshow(final_image_with_point)\n",
        "except:\n",
        "  cv2.imshow(final_image_with_point)\n"
      ],
      "metadata": {
        "id": "7DCinZUd9xk3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}